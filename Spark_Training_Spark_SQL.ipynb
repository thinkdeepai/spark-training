{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Training -  Spark SQL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinkdeepai/spark-training/blob/main/Spark_Training_Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VnDqkZr8stY",
        "outputId": "4233d12a-d770-49f4-d747-ac7149a470f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download and install dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -q pyngrok"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/681.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/681.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ic3beXT8z6Q"
      },
      "source": [
        "import os\n",
        "\n",
        "# set the environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop2.7\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki0-v6VO80cO"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "dxevjbJ-R3dI",
        "outputId": "c2aa9860-584e-476e-e698-70f2e02a6492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe73278c190>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ed24fd9528b1:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SYFOMQU82Kf",
        "outputId": "dadaafd2-d440-40f5-86dc-b402778cbc3a"
      },
      "source": [
        "# For this you need to create a free account on ngrok.com and enter your auth token below (replace AUHT_TOKEN)\n",
        "NGROK_AUTH_TOKEN = \"2CXOq3cZSdyxDgL97aR8zRo69KF_2PiUKLMyzpcxqq5TuLYQf\"\n",
        "\n",
        "# Export Spark history URL\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "\n",
        "# Open a TCP ngrok tunnel to the SSH server\n",
        "connection_string = ngrok.connect(4050, \"http\").public_url\n",
        "\n",
        "print(f\"Link to Spark History: {connection_string}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-29T08:32:44+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Link to Spark History: https://fb4f-34-23-60-174.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8FyiJT9YFd",
        "outputId": "a1326312-4de7-4968-9b6b-bb6e8a1e398c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Please download the data here:\n",
        "# https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json\n",
        "!wget https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-29 08:33:23--  https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73 [text/plain]\n",
            "Saving to: ‘people.json’\n",
            "\n",
            "\rpeople.json           0%[                    ]       0  --.-KB/s               \rpeople.json         100%[===================>]      73  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-29 08:33:23 (2.48 MB/s) - ‘people.json’ saved [73/73]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfdg_NT784td",
        "outputId": "69369fed-7802-4a96-cb2a-890ec5c18cd0"
      },
      "source": [
        "df = spark.read.json(\"people.json\")\n",
        "# Displays the content of the DataFrame to stdout\n",
        "df.show()\n",
        "# +----+-------+\n",
        "# | age|   name|\n",
        "# +----+-------+\n",
        "# |null|Michael|\n",
        "# |  30|   Andy|\n",
        "# |  19| Justin|\n",
        "# +----+-------+\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "|  25|  Kelly|\n",
            "|  36|Stephen|\n",
            "|  16| Andrew|\n",
            "+----+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wB73_Yg9HHm",
        "outputId": "76ebf54b-dbb0-4f20-ce76-1db0ef3f9b9a"
      },
      "source": [
        "# $example on:untyped_ops$\n",
        "# spark, df are from the previous example\n",
        "# Print the schema in a tree format\n",
        "df.printSchema()\n",
        "# root\n",
        "# |-- age: long (nullable = true)\n",
        "# |-- name: string (nullable = true)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeCTkGa293AU",
        "outputId": "e71da6c4-2309-4feb-ae0a-1851a2f5516f"
      },
      "source": [
        "# Select only the \"name\" column\n",
        "df.select(\"name\").show()\n",
        "# +-------+\n",
        "# |   name|\n",
        "# +-------+\n",
        "# |Michael|\n",
        "# |   Andy|\n",
        "# | Justin|\n",
        "# +-------+"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "|  Kelly|\n",
            "|Stephen|\n",
            "| Andrew|\n",
            "+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-NJbZ0--BcI"
      },
      "source": [
        "#  *** TODO Select everybody, but increment the age by 1\n",
        "df.select(df['name'], df['age'] + 1).show()\n",
        "# +-------+---------+\n",
        "# |   name|(age + 1)|\n",
        "# +-------+---------+\n",
        "# |Michael|     null|\n",
        "# |   Andy|       31|\n",
        "# | Justin|       20|\n",
        "# +-------+---------+\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX2NJtkY-DxI",
        "outputId": "93334101-2426-4ff0-c822-a80225f62564"
      },
      "source": [
        "# *** TODO Select people older than 21\n",
        "df.filter(df['age'] > 21).show()\n",
        "# +---+----+\n",
        "# |age|name|\n",
        "# +---+----+\n",
        "# | 30|Andy|\n",
        "# +---+----+\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "|age|   name|\n",
            "+---+-------+\n",
            "| 30|   Andy|\n",
            "| 25|  Kelly|\n",
            "| 36|Stephen|\n",
            "+---+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjkE9GG6-FWj",
        "outputId": "46d184dc-f6cf-49ed-db93-08d237da973e"
      },
      "source": [
        "# *** TODO Count people by age\n",
        "df.groupBy(\"age\").count().show()\n",
        "# +----+-----+\n",
        "# | age|count|\n",
        "# +----+-----+\n",
        "# |  19|    1|\n",
        "# |null|    1|\n",
        "# |  30|    1|\n",
        "# +----+-----+\n",
        "# $example off:untyped_ops$\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|  25|    1|\n",
            "|null|    1|\n",
            "|  36|    1|\n",
            "|  30|    1|\n",
            "|  16|    1|\n",
            "+----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmwDjS7A-IB3",
        "outputId": "080edfbc-a986-45bb-83ac-54869c81b09d"
      },
      "source": [
        "# $example on:run_sql$\n",
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
        "sqlDF.show()\n",
        "# +----+-------+\n",
        "# | age|   name|\n",
        "# +----+-------+\n",
        "# |null|Michael|\n",
        "# |  30|   Andy|\n",
        "# |  19| Justin|\n",
        "# +----+-------+\n",
        "# $example off:run_sql$\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "|  25|  Kelly|\n",
            "|  36|Stephen|\n",
            "|  16| Andrew|\n",
            "+----+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIwdj3AU-J3m"
      },
      "source": [
        "# $example on:global_temp_view$\n",
        "# Register the DataFrame as a global temporary view\n",
        "df.createGlobalTempView(\"people\")\n",
        "\n",
        "# Global temporary view is tied to a system preserved database `global_temp`\n",
        "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
        "\n",
        "# +----+-------+\n",
        "# | age|   name|\n",
        "# +----+-------+\n",
        "# |null|Michael|\n",
        "# |  30|   Andy|\n",
        "# |  19| Justin|\n",
        "# +----+-------+\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRVV8mHV-Q-S"
      },
      "source": [
        "# Global temporary view is cross-session\n",
        "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
        "# +----+-------+\n",
        "# | age|   name|\n",
        "# +----+-------+\n",
        "# |null|Michael|\n",
        "# |  30|   Andy|\n",
        "# |  19| Justin|\n",
        "# +----+-------+\n",
        "# $example off:global_temp_view$"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8J3J6SQTGeB",
        "outputId": "a0cab564-7830-4a9f-ce2e-2b4a1dac3928"
      },
      "source": [
        "# spark.sql(\"SELECT name FROM global_temp.people WHERE age >= 13 AND age <= 19\").show()\n",
        "df.select(\"name\").where(\"age >= 13 AND age <= 19\").show()\n",
        "df.select(\"name\").where(\"age <= 19\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "|Justin|\n",
            "|Andrew|\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "|Justin|\n",
            "|Andrew|\n",
            "+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2HCT37H-k8H",
        "outputId": "b1c891ba-abf2-440a-a1f1-128809648f4b"
      },
      "source": [
        "# teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
        "# teenagerNamesDF = spark.sql(\"SELECT avg(age) FROM people\")\n",
        "from pyspark.sql.functions import avg\n",
        "# df.agg({\"age\" : \"avg\"}).show()\n",
        "df.select(avg(\"age\")).show()\n",
        "# teenagerNamesDF.show()\n",
        "# +------+\n",
        "# |  name|\n",
        "# +------+\n",
        "# |Justin|\n",
        "# +------+\n",
        "# $example off:basic_parquet_example$"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|avg(age)|\n",
            "+--------+\n",
            "|    25.2|\n",
            "+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3FCiVRz_jl6"
      },
      "source": [
        "# Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
        "# an RDD[String] storing one JSON object per string\n",
        "jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
        "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
        "otherPeople = spark.read.json(otherPeopleRDD)\n",
        "\n",
        "# otherPeople.show()\n",
        "# +---------------+----+\n",
        "# |        address|name|\n",
        "# +---------------+----+\n",
        "# |[Columbus,Ohio]| Yin|\n",
        "# +---------------+----+\n",
        "# $example off:json_dataset$"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkKKnqFtAQIj"
      },
      "source": [
        "# Using Parquet\n",
        "\n",
        "df.write.parquet(\"people.parquet\")\n",
        "\n",
        "# Read in the Parquet file created above.\n",
        "# Parquet files are self-describing so the schema is preserved.\n",
        "# The result of loading a parquet file is also a DataFrame.\n",
        "parquetFile = spark.read.parquet(\"people.parquet\")\n",
        "\n",
        "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
        "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
        "\n",
        "# ***** TODO\n",
        "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "teenagers.show()\n",
        "# +------+\n",
        "# |  name|\n",
        "# +------+\n",
        "# |Justin|\n",
        "# +------+\n",
        "# $example off:basic_parquet_example$"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlAKoFSWbyGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6adfe0-34e4-4d45-f555-d3cdcfa567f4"
      },
      "source": [
        "# Inferring the Schema Using Reflection\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Load a text file and convert each line to a Row.\n",
        "lines = sc.textFile(\"people.txt\")\n",
        "\n",
        "parts = lines.map(lambda l: l.split(\",\"))\n",
        "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
        "\n",
        "# Infer the schema, and register the DataFrame as a table.\n",
        "schemaPeople = spark.createDataFrame(people)\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "# The results of SQL queries are Dataframe objects.\n",
        "# rdd returns the content as a :class:`pyspark.RDD` of :class:`Row`.\n",
        "teenagers.show()\n",
        "\n",
        "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
        "\n",
        "for name in teenNames:\n",
        "    print(name)\n",
        "# Name: Justin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "|Justin|\n",
            "|Andrew|\n",
            "+------+\n",
            "\n",
            "Name: Justin\n",
            "Name: Andrew\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUBjG3H7r8ye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bd7bcf-e69d-4736-8e58-c358f870dc5c"
      },
      "source": [
        "# ***** TODO\n",
        "# Try adding : \".explain(extended=True)\" to teenagers or any SparkSQL query\n",
        "# Observe the streaming query plans generated for the query\n",
        "\n",
        "# teenagers.explain(extended=True)\n",
        "# df.explain(extended=True)\n",
        "df.select(avg(\"age\")).explain(extended=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [avg('age) AS avg(age)#625]\n",
            "+- Relation[age#140L,name#141] json\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "avg(age): double\n",
            "Aggregate [avg(age#140L) AS avg(age)#625]\n",
            "+- Relation[age#140L,name#141] json\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [avg(age#140L) AS avg(age)#625]\n",
            "+- Project [age#140L]\n",
            "   +- Relation[age#140L,name#141] json\n",
            "\n",
            "== Physical Plan ==\n",
            "*(2) HashAggregate(keys=[], functions=[avg(age#140L)], output=[avg(age)#625])\n",
            "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#902]\n",
            "   +- *(1) HashAggregate(keys=[], functions=[partial_avg(age#140L)], output=[sum#629, count#630L])\n",
            "      +- FileScan json [age#140L] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/content/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}